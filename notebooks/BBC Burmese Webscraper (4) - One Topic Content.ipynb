{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b73c364e-aa87-4e3f-a03b-5ede253d83e1",
   "metadata": {},
   "source": [
    "# BBC Burmese Webscraper (4): Scraping Headers, Dates, and Contents of Multiple Pages\n",
    "by <a href=\"https://www.linkedin.com/in/la-wun-nannda-b047681b5/\">`La Wun Nannda`</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4bbbfa-9e6a-4259-a137-6b5671e21cd4",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fceff4c2-e6d2-4b51-868b-1c1e50f5f490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from bs4 import BeautifulSoup # this module helps in web scrapping\n",
    "import requests  # this module helps us to download a web page\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b76c31c-765b-461c-8c43-32b46a01a978",
   "metadata": {},
   "source": [
    "## Functions for One Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00c42acc-1d72-42f7-979e-2b28994c3c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get soup with URL input\n",
    "def web_scraper(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html5lib')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6fe67f4-57c5-407e-a394-619dba457c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get specific elements within a soup\n",
    "def soup_parser(soup):\n",
    "    news_headers_soup = soup.find_all(\"a\", {\"class\":\"focusIndicatorDisplayBlock\"})\n",
    "    datetime_soup = soup.find_all(\"time\", {\"class\":\"promo-timestamp\"})\n",
    "    return news_headers_soup, datetime_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb8e90e0-c110-4f36-b947-96c89c44deaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract Burmese content from a content url\n",
    "def content_scraper(content_url, soup):\n",
    "    burmese_content = \"\"\n",
    "    alphabets = ['a', 'b', 'c', 'd', 'e',\n",
    "                'f', 'g', 'h', 'i', 'j',\n",
    "                'k', 'l', 'm', 'n', 'o',\n",
    "                'p', 'q', 'r', 's', 't',\n",
    "                'u', 'v', 'w', 'x', 'y',\n",
    "                'z']\n",
    "    \n",
    "    symbols = [\"\\'\", \"(\", \")\"]\n",
    "    \n",
    "    for p_element in soup.find_all(\"p\"):\n",
    "        try: # None Type can cause error\n",
    "            content = p_element.string.strip()\n",
    "            for char in content:\n",
    "                if (char.lower() in alphabets) or (char in symbols): # do not add non-Burmese characters or symbols\n",
    "                    continue\n",
    "                burmese_content += char # add Burmese characters only\n",
    "        except:\n",
    "            pass\n",
    "    return burmese_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66fa3078-db10-473d-b8a7-3c9112ada2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create lists for a page\n",
    "'''Lists are to be appended to original ones'''\n",
    "def list_append_per_page(news_headers_soup, datetime_soup):\n",
    "    news_headers_per_page = []\n",
    "    datetime_per_page = []\n",
    "    contents_per_page = []\n",
    "    \n",
    "    if len(news_headers_soup)==len(datetime_soup): # each header should have a corresponding date\n",
    "        \n",
    "        for i in range(len(news_headers_soup)): # get index of headers for one page\n",
    "    \n",
    "            # list 1 for multiple headers in a page\n",
    "            try: # for news headers without video tag # video tagged ones will cause errors\n",
    "                news_headers_per_page.append(news_headers_soup[i].string.strip()) # convert 'BeautifulSoup string' to 'Python string' # add content to list 1\n",
    "            except AttributeError: # # for news headers with video tag\n",
    "                '''list() is used to convert 'BeautifulSoup tag' object to 'list' to enable iteration'''\n",
    "                news_headers_per_page.append(list(news_headers_soup[i].span)[1].strip()) # convert 'BeautifulSoup string' to 'Python string' # add content to list 1\n",
    "    \n",
    "            # list 2 for date and time in a page\n",
    "            datetime_per_page.append(datetime_soup[i].string.strip()) # convert 'BeautifulSoup string' to 'Python string' # add content to list 2\n",
    "\n",
    "            # list 3 for contents of all headers in a page (contents of multiple headers)\n",
    "            content_url = news_headers_soup[i].attrs['href'] # get a link from 'n' element\n",
    "            content_soup = web_scraper(content_url) # pass the link to create a new soup\n",
    "            content_per_header = content_scraper(content_url, content_soup) # this new soup is used for content scraping\n",
    "            contents_per_page.append(content_per_header)\n",
    "        \n",
    "        if (len(news_headers_per_page)==len(news_headers_soup)) & (len(datetime_per_page)==len(datetime_soup)) & (len(contents_per_page)!=0): # if everything is added to two lists\n",
    "            return news_headers_per_page, datetime_per_page, contents_per_page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936fff06-bb6d-4674-a0a9-3e9a899c4495",
   "metadata": {},
   "source": [
    "## Functions for Multiple Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da1e6acf-8a0d-4cad-ae77-e370388b6f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get next page url\n",
    "def navigate_next_page(web_url, soup):\n",
    "    next_page_soup = soup.find(\"a\", {\"aria-labelledby\":\"pagination-next-page\", \"class\":\"focusIndicatorOutlineBlack\", \"href\":True})\n",
    "    complete_url = web_url + next_page_soup.attrs['href']\n",
    "    return complete_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba09d7a1-a1fd-48c8-9bb9-d945bae0909d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the last page index\n",
    "def get_max_page(soup):\n",
    "    last_page_soup = soup.find_all(\"a\", {\"class\":\"focusIndicatorOutlineBlack\", \"href\":True})[-2]\n",
    "    last_page_index = int(last_page_soup.string)\n",
    "    return last_page_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9380caaa-01c6-4866-956e-3bf52f0dff17",
   "metadata": {},
   "source": [
    "## Function for Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bea827f-44d8-4cc3-954c-3d256a6ed3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to produce a spreadsheet\n",
    "def export_excel(first_list, second_list, third_list):\n",
    "    BBC = {}\n",
    "    BBC['News Header'] = first_list\n",
    "    BBC['Time'] = second_list\n",
    "    BBC['Content'] = third_list\n",
    "    df = pd.DataFrame({key:pd.Series(value) for key, value in BBC.items()})\n",
    "    df.to_excel('../spreadsheets/BBC_webscraped.xlsx', index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719819a5-c1ce-460e-8e25-cd6aec0091bd",
   "metadata": {},
   "source": [
    "## Scraping Every Page to Spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b94b09bf-39c8-4a7d-b3aa-bcfb26cc0d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the main function\n",
    "def main(web_url):\n",
    "    news_headers = []\n",
    "    datetime = []\n",
    "    contents = []\n",
    "\n",
    "    # for the first page\n",
    "    complete_url = web_url # initial url\n",
    "    soup = web_scraper(complete_url) # get soup\n",
    "    last_page_index = get_max_page(soup) # get last page index\n",
    "    news_headers_soup, datetime_soup = soup_parser(soup) # get specific elements in a soup\n",
    "    print(\"Total pages:\", last_page_index)\n",
    "    \n",
    "    # extract data\n",
    "    news_headers_per_page, datetime_per_page, contents_per_page = list_append_per_page(news_headers_soup, datetime_soup)\n",
    "\n",
    "    # append data to lists\n",
    "    news_headers += news_headers_per_page\n",
    "    datetime += datetime_per_page\n",
    "    contents += contents_per_page\n",
    "    \n",
    "    # from the second page to the last page\n",
    "    for i in range(last_page_index): \n",
    "        try:\n",
    "            print(complete_url)\n",
    "            complete_url = navigate_next_page(web_url, soup) # get next url\n",
    "            soup = web_scraper(complete_url) # get soup\n",
    "            news_headers_soup, datetime_soup = soup_parser(soup) # get specific elements in a soup\n",
    "\n",
    "            # extract data\n",
    "            news_headers_per_page, datetime_per_page, contents_per_page = list_append_per_page(news_headers_soup, datetime_soup)\n",
    "            \n",
    "            # append data to lists\n",
    "            news_headers += news_headers_per_page\n",
    "            datetime += datetime_per_page\n",
    "            contents += contents_per_page\n",
    "        \n",
    "        except AttributeError: # scraping the page after the last page will cause error\n",
    "            print(\"The end of the pages is reached.\")\n",
    "\n",
    "    return export_excel(news_headers, datetime, contents) # export the spreadsheet # return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae7fa40b-ac15-4029-b9f9-8ad7f809c7bf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages: 28\n",
      "https://www.bbc.com/burmese/topics/c9wpm0en9jdt\n",
      "https://www.bbc.com/burmese/topics/c9wpm0en9jdt?page=2\n",
      "https://www.bbc.com/burmese/topics/c9wpm0en9jdt?page=3\n",
      "https://www.bbc.com/burmese/topics/c9wpm0en9jdt?page=4\n",
      "https://www.bbc.com/burmese/topics/c9wpm0en9jdt?page=5\n",
      "https://www.bbc.com/burmese/topics/c9wpm0en9jdt?page=6\n",
      "https://www.bbc.com/burmese/topics/c9wpm0en9jdt?page=7\n",
      "https://www.bbc.com/burmese/topics/c9wpm0en9jdt?page=8\n",
      "https://www.bbc.com/burmese/topics/c9wpm0en9jdt?page=9\n",
      "https://www.bbc.com/burmese/topics/c9wpm0en9jdt?page=10\n",
      "https://www.bbc.com/burmese/topics/c9wpm0en9jdt?page=11\n",
      "https://www.bbc.com/burmese/topics/c9wpm0en9jdt?page=12\n",
      "https://www.bbc.com/burmese/topics/c9wpm0en9jdt?page=13\n",
      "https://www.bbc.com/burmese/topics/c9wpm0en9jdt?page=14\n",
      "https://www.bbc.com/burmese/topics/c9wpm0en9jdt?page=15\n",
      "https://www.bbc.com/burmese/topics/c9wpm0en9jdt?page=16\n",
      "https://www.bbc.com/burmese/topics/c9wpm0en9jdt?page=17\n",
      "https://www.bbc.com/burmese/topics/c9wpm0en9jdt?page=18\n",
      "https://www.bbc.com/burmese/topics/c9wpm0en9jdt?page=19\n",
      "https://www.bbc.com/burmese/topics/c9wpm0en9jdt?page=20\n",
      "https://www.bbc.com/burmese/topics/c9wpm0en9jdt?page=21\n",
      "https://www.bbc.com/burmese/topics/c9wpm0en9jdt?page=22\n",
      "https://www.bbc.com/burmese/topics/c9wpm0en9jdt?page=23\n",
      "https://www.bbc.com/burmese/topics/c9wpm0en9jdt?page=24\n",
      "https://www.bbc.com/burmese/topics/c9wpm0en9jdt?page=25\n",
      "https://www.bbc.com/burmese/topics/c9wpm0en9jdt?page=26\n",
      "https://www.bbc.com/burmese/topics/c9wpm0en9jdt?page=27\n",
      "https://www.bbc.com/burmese/topics/c9wpm0en9jdt?page=28\n",
      "The end of the pages is reached.\n"
     ]
    }
   ],
   "source": [
    "df = main(\"https://www.bbc.com/burmese/topics/c9wpm0en9jdt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3124ed79-7166-40fc-b211-d78f8062457c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News Header</th>\n",
       "      <th>Time</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ဗီယက်နမ်ခေါင်းဆောင် ငုယင်ဖူချောင်း အသက် ၈ဝ မှာ...</td>\n",
       "      <td>၁၉ ဇူလိုင် ၂၀၂၄</td>\n",
       "      <td>ဗီယက်နမ်ရဲ့ သက်တမ်းရှည် ကွန်မြူနစ်ပါတီခေါင်းဆေ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‘ကျွန်တော်လက်ထဲ ဘာလက်နက်မှမရှိဘူး’- ပစ်သတ်ခံခဲ...</td>\n",
       "      <td>၁၈ ဇူလိုင် ၂၀၂၄</td>\n",
       "      <td>အဘူ ဆာယိဒ်ဟာ ဘင်္ဂလားဒေ့ရှ်နိုင်ငံက ကျောင်းသား...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ဘန်ကောက် ဟိုတယ်ထဲ သေဆုံးသူ ၆ ဦးရဲ့သွေးထဲမှာ ဆိ...</td>\n",
       "      <td>၁၇ ဇူလိုင် ၂၀၂၄</td>\n",
       "      <td>ထိုင်းနိုင်ငံ၊ ဘန်ကောက်မြို့က နာမည်ကျော် ဟိုတယ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ထိုင်းဗီဇာမလိုဘဲ ပြည်တွင်းဝင်ခွင့် နိုင်ငံ ၃၀ ...</td>\n",
       "      <td>၁၃ ဇူလိုင် ၂၀၂၄</td>\n",
       "      <td>ထိုင်းမှာ တနင်္လာနေ့ကစပြီး ဗီဇာမလိုဘဲ ပြည်တွင်...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ရိုးမရဲ့ ဘီလျံနာသူဌေးကြီး ဆာ့ချ်ပန်းကို စစ်ကော...</td>\n",
       "      <td>၁၁ ဇူလိုင် ၂၀၂၄</td>\n",
       "      <td>ရိုးမဘဏ်နဲ့ ရိုးမအုပ်စုတည်ထောင်သူ ဦးသိမ်းဝေ ခ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>ကိုဗစ်ကြောင့် ထိုင်းမှာ ပိတ်ထားတာတွေကို ပြန်ဖွင့်</td>\n",
       "      <td>၈ မေ ၂၀၂၀</td>\n",
       "      <td>ထိုင်းနိုင်ငံမှာ ကိုဗစ်၁၉ ကူးစက်မှုတွေများလာတဲ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>ရှမ်းပြည်က ပြောင်းဖူးတွေ မြစ်ထဲ ပစ်ချနေရတဲ့ နေ...</td>\n",
       "      <td>၈ မေ ၂၀၂၀</td>\n",
       "      <td>မြန်မာ-တရုတ်နယ်စပ်မှာ ပြောင်းဖူးသွားရောင်းတဲ့တ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>အိန္ဒိယမှာ ကိုရိုနာဗိုင်းရပ်စ် ကူးစက်မှု ထိုးတက်</td>\n",
       "      <td>၅ မေ ၂၀၂၀</td>\n",
       "      <td>အိန္ဒိယမှာ တစ်နေ့တည်း ကူးစက်မှု အများဆုံး အဖြစ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>ကိုဗစ်ကြောင့်တန်နဲ့ချီ ပျက်စီးခဲ့ရတဲ့ တရုတ်နယ်...</td>\n",
       "      <td>၂၈ ဧပြီ ၂၀၂၀</td>\n",
       "      <td>မြန်မာနိုင်ငံ ကနေ တရုတ်နိုင်ငံကို နယ်စပ်ကနေ တင...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>တစ်ပတ်အတွင်း ဘာတွေဖြစ်ခဲ့လဲ</td>\n",
       "      <td>၂၈ ဧပြီ ၂၀၂၀</td>\n",
       "      <td>ပြီးခဲ့တဲ့တစ်ပတ်အတွင်း လှုပ်ခတ်မှုအရှိဆုံးသတင်...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>671 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           News Header             Time  \\\n",
       "0    ဗီယက်နမ်ခေါင်းဆောင် ငုယင်ဖူချောင်း အသက် ၈ဝ မှာ...  ၁၉ ဇူလိုင် ၂၀၂၄   \n",
       "1    ‘ကျွန်တော်လက်ထဲ ဘာလက်နက်မှမရှိဘူး’- ပစ်သတ်ခံခဲ...  ၁၈ ဇူလိုင် ၂၀၂၄   \n",
       "2    ဘန်ကောက် ဟိုတယ်ထဲ သေဆုံးသူ ၆ ဦးရဲ့သွေးထဲမှာ ဆိ...  ၁၇ ဇူလိုင် ၂၀၂၄   \n",
       "3    ထိုင်းဗီဇာမလိုဘဲ ပြည်တွင်းဝင်ခွင့် နိုင်ငံ ၃၀ ...  ၁၃ ဇူလိုင် ၂၀၂၄   \n",
       "4    ရိုးမရဲ့ ဘီလျံနာသူဌေးကြီး ဆာ့ချ်ပန်းကို စစ်ကော...  ၁၁ ဇူလိုင် ၂၀၂၄   \n",
       "..                                                 ...              ...   \n",
       "666  ကိုဗစ်ကြောင့် ထိုင်းမှာ ပိတ်ထားတာတွေကို ပြန်ဖွင့်        ၈ မေ ၂၀၂၀   \n",
       "667  ရှမ်းပြည်က ပြောင်းဖူးတွေ မြစ်ထဲ ပစ်ချနေရတဲ့ နေ...        ၈ မေ ၂၀၂၀   \n",
       "668   အိန္ဒိယမှာ ကိုရိုနာဗိုင်းရပ်စ် ကူးစက်မှု ထိုးတက်        ၅ မေ ၂၀၂၀   \n",
       "669  ကိုဗစ်ကြောင့်တန်နဲ့ချီ ပျက်စီးခဲ့ရတဲ့ တရုတ်နယ်...     ၂၈ ဧပြီ ၂၀၂၀   \n",
       "670                        တစ်ပတ်အတွင်း ဘာတွေဖြစ်ခဲ့လဲ     ၂၈ ဧပြီ ၂၀၂၀   \n",
       "\n",
       "                                               Content  \n",
       "0    ဗီယက်နမ်ရဲ့ သက်တမ်းရှည် ကွန်မြူနစ်ပါတီခေါင်းဆေ...  \n",
       "1    အဘူ ဆာယိဒ်ဟာ ဘင်္ဂလားဒေ့ရှ်နိုင်ငံက ကျောင်းသား...  \n",
       "2    ထိုင်းနိုင်ငံ၊ ဘန်ကောက်မြို့က နာမည်ကျော် ဟိုတယ...  \n",
       "3    ထိုင်းမှာ တနင်္လာနေ့ကစပြီး ဗီဇာမလိုဘဲ ပြည်တွင်...  \n",
       "4    ရိုးမဘဏ်နဲ့ ရိုးမအုပ်စုတည်ထောင်သူ ဦးသိမ်းဝေ ခ ...  \n",
       "..                                                 ...  \n",
       "666  ထိုင်းနိုင်ငံမှာ ကိုဗစ်၁၉ ကူးစက်မှုတွေများလာတဲ...  \n",
       "667  မြန်မာ-တရုတ်နယ်စပ်မှာ ပြောင်းဖူးသွားရောင်းတဲ့တ...  \n",
       "668  အိန္ဒိယမှာ တစ်နေ့တည်း ကူးစက်မှု အများဆုံး အဖြစ...  \n",
       "669  မြန်မာနိုင်ငံ ကနေ တရုတ်နိုင်ငံကို နယ်စပ်ကနေ တင...  \n",
       "670  ပြီးခဲ့တဲ့တစ်ပတ်အတွင်း လှုပ်ခတ်မှုအရှိဆုံးသတင်...  \n",
       "\n",
       "[671 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the dataframe\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
